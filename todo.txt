2025-02-05
- introduced Domain (wiki/plots),
- used it in combined_app, need to test and see what else need changing
2025-02-04
- switched from articles to document to facilitate support of plots
2025-02-03
- move class definitions to a types package
- rename app_config.py combined_config.py

2025-01-30
+ lookup.py for segments fixed, uses combined_service
+ extend lookup.py to do RAG
- look into plots - search
- look into plots - RAG
- look into articles search / rag

2025-01-28
+ see that wiki search works
+ see that wiki RAG works
- see that plots search works
- see that plots RAG works
- make

2025-01-26
- revive unit tests
- revive coverage
- gen and encode segments


2025-01-10
- unittest coverage
- encode plots and wiki
- search
- plan forward
- look for a streaming / updatedable case

2025-01-03

New Segment Builder:
uses PlotData:
  segmentize_plots
    calls segmentize_plot with plot_segments: List[PlotData]

  segmentize_plot
    get_balanced_seg_length(length)
    plot_reader(plot_data) : uses offset and length -> bytes

-> to generalized we can take a list of bytes




2024-12-29
  + added plot
  + make code reusable, for example: overlap_setter, now used by segment_builder
    and hopefully with new_segment_builder
    - add overlaps to new_segment_builder
  + added new_segment_builder which might be a simplified version of segment_builder,
    - look for opportunities to reuse code
2024-12-23
  + improve unittest coverage (now at 100%)

2024-12-22
  ? extend unittest coverage ?

2024-12-21
+ added combined_service
+ cleaned up combined_app
+ supporting article, segments, and a select box
+ other improvements:
  + length of search results (chars, tokens)
  + time since update (using dayjs)


2024-12-20

+ read a blog post, a paper, and an article
- read https://platform.openai.com/docs/guides/prompt-generation

+ openai releases gpt o3


2024-12-19
+ refactor rag_app and search_app -- created combined_app
- move from articles to segments - shorter prompts to the AI
  + converted lookup.py
  - work on combined_app.py    ZZZZZZZ

2024-12-18
+ load normalized float16 embeddings
+ fix embed config to load embeddings and the one to encode query
+ using 1500 now - greater number of segments
+ cleanup
+ base store added
+ search performance improved
+ logging improved
- refactor rag_app and search_app


2024-12-17
+ quantization
+ check similarity

- check timing of 1500
- check timing of OpenAI API
- try to split prompt to query/prompt by an API call
- fix threshold / max / k logic


2024-12-12


2024-12-11
+ reduce dimension
+ create 1500 segments
+ improve performance of k nearest
  + normalize embeddings before dist
  + use polar

2024-12-07

later - preserve segment index / weight ...
+ already working - look into the enhancement of avoiding tiny segments, see below
later - more central config
  - aggregation type
  - big / small config
+ using flat segments and article during lookup
- consider takeing path to store from the class

2024-12-06
- preserve segment index / weight ...
- look into the enhancement of avoiding tiny segments, see below
- more central config
  - aggregation type
  - big / small config
- using flat segments and article during lookup
- consider takeing path to store from the class

2024-12-06
+ increase unit test coverage
+ fix embedding encoding, need to use prompt with task: search_document:, search_query:
+ try medium size segmentation, large segments don't work well with short queries
/ not now: increase overlaps from 10% to 15%
- preserve segment index / weight ...
- look into the enhancement of avoiding tiny segments, see below
- more central config
  - aggregation type
  - big / small config
- using flat segments and article during lookup
- consider takeing path to store from the class

+ notoriously a log statement, doesn't use the warning mechanism, therefore difficult to suppress
/ deal with:
  huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
  To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

2024-12-05

2024-12-04
+ flat segment / article file, during search there is no need for all fine objects
  + flat ext segments: article, offset, length
  + flat article: header section(offset, length), body section(offset, length)
+ lazy loading of articles
+ get rid of delay in element

2024-12-03


2024-11-25

+ create a simple Web UI
+ simple search by segment
+ search by article
  + rudimentary
  ? extend
- RAG
- improve segmentation:
  - if current segment doesn't have room for the next section but it is
    mostly empty, split the next section and fill up the segment

+ added incremental
+ coverage 98%

2024-11-20

+ check coverage, add unit tests
+ make encode_segment incremental

+ encode segments, store embeddings
+ switched to UUIDs and resolve_dependencies


2024-11-19
+ continue build_segments

+ xdata added
  saving constructs:
    save length instead of bytes
    save ids instead of content of referenced object
    save referenced objects before referencing to the them
    save a flat list of objects

    load
    iterate over a flat list of objects
    convert offset/length to offset/bytes
    replace ids with the objects
    call hierarchy_from_data (or from json - TBD)


  save(articles):
    for article in articles:
        article.to_xdata(context)



? register a reset listener with a contained object so we can reset the container

+ test first / last extended article_segments
+ encoding utils -- test sign_mode 0 and -1

1. Min-threshold, soft-max, hard-max, and overlaps.
2. Token-to-character conversion (~3.9 per token).
3. Segmentation and encoding on cleaned text while
   tracking raw and cleaned text lengths separately.



max_len = 1024

max = n

if index == 0:
  i_overlap = 0
  f_overlap = 0.1 N

else:
  i_overlap = 0.1 N
  f_overlap = 0.1 N

soft_max = 0.8 N


while True:
  if can add fragment to segment:
    add fragment to segment
    break
  else:
    if segment is empty:
    segment = new segment

article_segments = []
segment = Segment()
article_segments.append(segment)

while section = sections.shift():
    if segment.clean_length + section.clean_length <= max_len:

      # simple case, just add section
      add section to segment
      continue

    if current segment length == 0:
      # if first section is too long, split it
      first, remainder = section.split(max-len)
      segment.append(first)
      sections.unshift(remainder)
      continue

    # add overlap to the end of the current segment
    overlap = min(max_overlap, max_len - current segment length)
    segment.append(section[-overlap:])

    # create a new segment






for a_index, article in articles:
  segment_length = 0
  for p_index, paragraph in article:
    soft_max = 0.9 * N if p_index == 0 else 0.8 * soft_max

    if segment_length + len(paragraph) < hard_max:
      segment.append(paragraph)


    if segment_length + len(paragraph) < soft_max:
      segment_length += len(paragraph)
    elif segment_length + len(paragraph) < hard_max:
      segment_length += len(paragraph)
    elif segment_length == 0:
      segment_length = paragraph[:soft_max]
      segment_length = paragraph[soft_max:]

      segment = new segment(paragraph)
      segment_length = len(paragraph)
      yield segment_length, article, p_index




indexer(Dispatcher):
  def add_paragraph(paragraph)
    self.articles[-1].append_paragraph(paragraph)
    super().handle(paragraph)

validator(Dispatcher):
    def __init__(self, dispatcher):
        self.dispatcher = dispatcher

    def validate(self, paragraph):
      self.validate(paragraph)
      super().handle(paragraph)


def main_simple_in_process():
  indexer = Indexer()
  validator = Validator()
  dumper = Dumper()

  indexer.register_handler(lambda par: validator.validate(par))
  indexer.register_handler(lambda par: dumper.dump(par))


def main_queued_threaded():
  indexer = Indexer()
  validator = Validator()
  dumper = Dumper()

  validator_stage = QueuedStage()
  validator_stage.register_handler(lambda par: validator.validate(par))
  indexer.register_handler(validator_stage)

  dumper_stage = QueuedStage()
  dumper_stage.register_handler(lambda par: dumper.dump(par))
  indexer.register_handler(dumper_stage)

  validator_stage.start()
  dumper_stage.start()

  indexer.index(path)

  validator_stage.stop()
  dumper_stage.stop()



########################################################


IndexBuilder:
    def handle_element(element):
        pass

SnippetValidator:
    def __init__(self, args: argparse.Namespace):
        self.args = args

    def validate_element(element):
        self._validate_element(element)
        self.handle_element(element)

    def handle_element(element):
        pass

IndexDumper:
    def dump_element(element):
        self._dump_element(element)
        self.handle_element(element)

    def dump_element(element):
        pass



def main():
  builder = IndexBuilder()

  if mode == "inline":
      validator = SnippetValidator(args)
      builder.register_handler(lambda element: validator.validate_element(element))

      dumper = IndexDumper(args)
      builder.register_handler(lambda element: dumper.dump_element(element))
  else:
      validator_stage = QueuedStage().delayed_initialize(lambda self: self.)

      validator = QueuedStage()
      validator.register_handler(SnippetValidator())

  dumper = IndexDumper()

  builder.register_handler(validator)
  builder.register_handler(dumper)


#####################################################
*** lookup

- main gets input from user, calls the lookup service, prints results
- lookup reads segments and embeddings, performs the lookup and returns results


#####################################################




2024-12-14 ================================================

Morphing Embeddings

capture how embeddings need to be manipulated before use.
standard pipeline (morph_embeddings.py)

1. reduce dimension from 768 to 512, 256, 128, 64
- current implementation tied to a model that supports matryoshka dimensions
-


Embeddings
- prefix
- max-len
- dim - 768, 512, 256, 128, 64  # None -> 768
- stype - float32, float16, int8, uint8  # None -> float32
- norm-type - float32, float16, int8, uint8  # None -> don't astype before norm


data/train_1
5000
768
float32
None



2024-12-16

What we want is:
- load stype (int8)





2024-12-21

Handle both segments and articles -- for now only flat (adapt to compound elements later)

article:
return an article-id / similarity tuple list

segment:
find segment return a segment-id / similarity tuple list

get element-results:

article:
get article
get article.header.text
element-result = similarity, article, header-text

segment:
get segment
get article
get article.header.text
element-result = similarity, segment, header-text + segment.text[:60] + "..."


2025-01-23

Stores:
  def get_uids_and_embeddings()


k_nearest_finder:
  - stores.uids_and_embeddings
  - stores.get_embeddings_article_ids

combined_service:
  - get_article(article_id)  # to be replaced with get_article_by_index
  - get_segment(segment_id)
  - get_article_by_index(article_index)

  * ignore get_article, replace it with get_article_by_index
  * get_segment is used to a) get the article (id) off the record, and b) get the segment text
  * add get_segment_text(segment_record) to stores

combined_app:
  - stores.background_load
  - KNearestFinder(stores, embed_config)

run_app:
  - stores.background_load
  - KNearestFinder(stores, embed_config)
  - stores.get_article(article_id) (--> get_article_by_index)

search_app:
  - ditto


1. load_embeddings can be replaced with EmbeddingStore.load_embeddings()
2. get_embeddings_article_ids:
     get segment records
     [segment_record.document_index for segment_record in segment_records]
3. segment_records_path = f"{path_prefix}_{max_len}_flat_segments.csv"
   segment_records = SegmentVerifier.read_segment_records(segment_records_path)




EmbeddingStore:
   - uids, embeddings = load_embeddings()
