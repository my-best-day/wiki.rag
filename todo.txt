2024-12-18
+ load normalized float16 embeddings
+ fix embed config to load embeddings and the one to encode query
+ using 1500 now - greater number of segments
+ cleanup
+ base store added
+ search performance improved
+ logging improved
- refactor rag_app and search_app


2024-12-17
+ quantization
+ check similarity

- check timing of 1500
- check timing of OpenAI API
- try to split prompt to query/prompt by an API call
- fix threshold / max / k logic


2024-12-12


2024-12-11
+ reduce dimension
+ create 1500 segments
+ improve performance of k nearest
  + normalize embeddings before dist
  + use polar

2024-12-07

later - preserve segment index / weight ...
+ already working - look into the enhancement of avoiding tiny segments, see below
later - more central config
  - aggregation type
  - big / small config
+ using flat segments and article during lookup
- consider takeing path to store from the class

2024-12-06
- preserve segment index / weight ...
- look into the enhancement of avoiding tiny segments, see below
- more central config
  - aggregation type
  - big / small config
- using flat segments and article during lookup
- consider takeing path to store from the class

2024-12-06
+ increase unit test coverage
+ fix embedding encoding, need to use prompt with task: search_document:, search_query:
+ try medium size segmentation, large segments don't work well with short queries
/ not now: increase overlaps from 10% to 15%
- preserve segment index / weight ...
- look into the enhancement of avoiding tiny segments, see below
- more central config
  - aggregation type
  - big / small config
- using flat segments and article during lookup
- consider takeing path to store from the class

+ notoriously a log statement, doesn't use the warning mechanism, therefore difficult to suppress
/ deal with:
  huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
  To disable this warning, you can either:
    - Avoid using `tokenizers` before the fork if possible
    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

2024-12-05

2024-12-04
+ flat segment / article file, during search there is no need for all fine objects
  + flat ext segments: article, offset, length
  + flat article: header section(offset, length), body section(offset, length)
+ lazy loading of articles
+ get rid of delay in element

2024-12-03


2024-11-25

+ create a simple Web UI
+ simple search by segment
+ search by article
  + rudimentary
  ? extend
- RAG
- improve segmentation:
  - if current segment doesn't have room for the next section but it is
    mostly empty, split the next section and fill up the segment

+ added incremental
+ coverage 98%

2024-11-20

+ check coverage, add unit tests
+ make encode_segment incremental

+ encode segments, store embeddings
+ switched to UUIDs and resolve_dependencies


2024-11-19
+ continue build_segments

+ xdata added
  saving constructs:
    save length instead of bytes
    save ids instead of content of referenced object
    save referenced objects before referencing to the them
    save a flat list of objects

    load
    iterate over a flat list of objects
    convert offset/length to offset/bytes
    replace ids with the objects
    call hierarchy_from_data (or from json - TBD)


  save(articles):
    for article in articles:
        article.to_xdata(context)



? register a reset listener with a contained object so we can reset the container

+ test first / last extended article_segments
+ encoding utils -- test sign_mode 0 and -1

1. Min-threshold, soft-max, hard-max, and overlaps.
2. Token-to-character conversion (~3.9 per token).
3. Segmentation and encoding on cleaned text while
   tracking raw and cleaned text lengths separately.



max_len = 1024

max = n

if index == 0:
  i_overlap = 0
  f_overlap = 0.1 N

else:
  i_overlap = 0.1 N
  f_overlap = 0.1 N

soft_max = 0.8 N


while True:
  if can add fragment to segment:
    add fragment to segment
    break
  else:
    if segment is empty:
    segment = new segment

article_segments = []
segment = Segment()
article_segments.append(segment)

while section = sections.shift():
    if segment.clean_length + section.clean_length <= max_len:

      # simple case, just add section
      add section to segment
      continue

    if current segment length == 0:
      # if first section is too long, split it
      first, remainder = section.split(max-len)
      segment.append(first)
      sections.unshift(remainder)
      continue

    # add overlap to the end of the current segment
    overlap = min(max_overlap, max_len - current segment length)
    segment.append(section[-overlap:])

    # create a new segment






for a_index, article in articles:
  segment_length = 0
  for p_index, paragraph in article:
    soft_max = 0.9 * N if p_index == 0 else 0.8 * soft_max

    if segment_length + len(paragraph) < hard_max:
      segment.append(paragraph)


    if segment_length + len(paragraph) < soft_max:
      segment_length += len(paragraph)
    elif segment_length + len(paragraph) < hard_max:
      segment_length += len(paragraph)
    elif segment_length == 0:
      segment_length = paragraph[:soft_max]
      segment_length = paragraph[soft_max:]

      segment = new segment(paragraph)
      segment_length = len(paragraph)
      yield segment_length, article, p_index




indexer(Dispatcher):
  def add_paragraph(paragraph)
    self.articles[-1].append_paragraph(paragraph)
    super().handle(paragraph)

validator(Dispatcher):
    def __init__(self, dispatcher):
        self.dispatcher = dispatcher

    def validate(self, paragraph):
      self.validate(paragraph)
      super().handle(paragraph)


def main_simple_in_process():
  indexer = Indexer()
  validator = Validator()
  dumper = Dumper()

  indexer.register_handler(lambda par: validator.validate(par))
  indexer.register_handler(lambda par: dumper.dump(par))


def main_queued_threaded():
  indexer = Indexer()
  validator = Validator()
  dumper = Dumper()

  validator_stage = QueuedStage()
  validator_stage.register_handler(lambda par: validator.validate(par))
  indexer.register_handler(validator_stage)

  dumper_stage = QueuedStage()
  dumper_stage.register_handler(lambda par: dumper.dump(par))
  indexer.register_handler(dumper_stage)

  validator_stage.start()
  dumper_stage.start()

  indexer.index(path)

  validator_stage.stop()
  dumper_stage.stop()



########################################################


IndexBuilder:
    def handle_element(element):
        pass

SnippetValidator:
    def __init__(self, args: argparse.Namespace):
        self.args = args

    def validate_element(element):
        self._validate_element(element)
        self.handle_element(element)

    def handle_element(element):
        pass

IndexDumper:
    def dump_element(element):
        self._dump_element(element)
        self.handle_element(element)

    def dump_element(element):
        pass



def main():
  builder = IndexBuilder()

  if mode == "inline":
      validator = SnippetValidator(args)
      builder.register_handler(lambda element: validator.validate_element(element))

      dumper = IndexDumper(args)
      builder.register_handler(lambda element: dumper.dump_element(element))
  else:
      validator_stage = QueuedStage().delayed_initialize(lambda self: self.)

      validator = QueuedStage()
      validator.register_handler(SnippetValidator())

  dumper = IndexDumper()

  builder.register_handler(validator)
  builder.register_handler(dumper)


#####################################################
*** lookup

- main gets input from user, calls the lookup service, prints results
- lookup reads segments and embeddings, performs the lookup and returns results


#####################################################




2024-12-14 ================================================

Morphing Embeddings

capture how embeddings need to be manipulated before use.
standard pipeline (morph_embeddings.py)

1. reduce dimension from 768 to 512, 256, 128, 64
- current implementation tied to a model that supports matryoshka dimensions
-


Embeddings
- prefix
- max-len
- dim - 768, 512, 256, 128, 64  # None -> 768
- stype - float32, float16, int8, uint8  # None -> float32
- norm-type - float32, float16, int8, uint8  # None -> don't astype before norm


data/train_1
5000
768
float32
None



2024-12-16

What we want is:
- load stype (int8)
